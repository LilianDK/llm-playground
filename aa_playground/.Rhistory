string = ""
for (x in 1:length(df)) {
string = glue("string{df[1,x]}")
}
string
df[1,1]
df[1,2]
df[2,1]
string = ""
for (x in 1:length(df)) {
string = glue("string{df[x,1]}")
}
string
length(df)
results(df)
length(results)
nrow(results)
string = ""
for (x in 1:nrow(results)) {
string = glue("string{df[x,1]}")
}
string
string = ""
string = glue("string{df[1,1]}")
string
string = ""
for (x in 1:nrow(results)) {
string = glue("{df[1,1]}")
}
string
string = ""
for (x in 1:nrow(results)) {
string = glue("{resutls[2,1]}")
}
string = ""
for (x in 1:nrow(results)) {
string = glue("{results[2,1]}")
}
string
string = ""
for (x in 1:nrow(results)) {
string = glue("{results[x,1]}")
}
string
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
string
summary = summary(token, string)
summary
source_python("api_clients/aa_summarization.py")
summary = summary(token, string)
summary
library(reticulate)
library(pdftools)
library(reticulate)
library(glue)
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
library(pdftools)
library(reticulate)
library(glue)
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzA4NH0.SyDVglcLc5FLLF86GV9z0D4WfxQF0uvAmeW8YlnG7to"
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
colnames(df) <- c("Text_chunk","page")
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
# Semantic search
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
results = df[vector,]
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
clear
clear()
clean()
library(pdftools)
library(reticulate)
library(glue)
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzA4NH0.SyDVglcLc5FLLF86GV9z0D4WfxQF0uvAmeW8YlnG7to"
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
colnames(df) <- c("Text_chunk","page")
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
# Semantic search
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
results = df[vector,]
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
string
qna = qna(token, string, query)
qna = qna(token, string, query)
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
source_python("api_clients/aa_qna.py")
source_python("api_clients/aa_qna.py")
library(pdftools)
library(reticulate)
library(glue)
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzA4NH0.SyDVglcLc5FLLF86GV9z0D4WfxQF0uvAmeW8YlnG7to"
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
colnames(df) <- c("Text_chunk","page")
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
# Semantic search
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
results = df[vector,]
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
string
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
View(tbl)
string
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
gsub(pattern, replacement, string, ignore.case=TRUE/FALSE)
gsub(“[\r\n]”, “”, string)
gsub("[\r\n]", "", string)
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
string = gsub("[\r\n]", "", string)
string
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
reticulate::py_last_error()
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzA4NH0.SyDVglcLc5FLLF86GV9z0D4WfxQF0uvAmeW8YlnG7to"
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
colnames(df) <- c("Text_chunk","page")
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
# Semantic search
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
results = df[vector,]
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
string
string = gsub("[\r\n]", "", string)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
View(qna)
qna[0]
qna$0
View(qna)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
reticulate::py_last_error()
qna[[1]]
qna[1]
View(qna)
qna[[1]]
reticulate::py_last_error()
qna[[0]]
reticulate::py_last_error()
qna[[0][0]]
qna[0][0]
typeof(qna)
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
reticulate::py_last_error()
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
reticulate::py_last_error()
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
qna[[2]]
localization = qna[[1]]
score = qna[[2]]
nlg = qna[[4]]
localization
score
nlg
nlg = trimws(qna[[4]])
nlg
string = trimws(gsub("[\r\n]", "", string))
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
View(results)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(pdftools)
library(reticulate)
library(glue)
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzA4NH0.SyDVglcLc5FLLF86GV9z0D4WfxQF0uvAmeW8YlnG7to"
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
#document = txt[1]
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
colnames(df) <- c("Text_chunk","page")
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
# Semantic search
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
results = df[vector,]
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
string = trimws(gsub("[\r\n]", "", string))
qna = qna(token, string, query)
localization = qna[[1]]
score = qna[[2]]
nlg = trimws(qna[[4]])
localization
score
nlg
runApp()
runApp()
renv::snapshot()
library(pdftools)
library(reticulate)
library(glue)
py_install("aleph-alpha-client")
py_install("Jinja2")
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_summarization.py")
token = ""
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/Testfile.pdf"
txt = pdf_text(pdf_file)
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzExMH0.waXyiiWVTYaiHkpW2fuTcFx9-R67j3_FZBpm0yfEUFg"
document = txt[1]
summary = summary(token, document)
summary
library(pdftools)
library(reticulate)
library(glue)
# Put the path to your python environment here
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
# Install and or load packages - set variables ---------------------------------
packages <- c("pdftools","reticulate","glue")
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
library(pdftools)
library(reticulate)
library(glue)
# Put the path to your python environment here
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("aleph-alpha-client")
py_install("Jinja2")
py_install("numpy")
py_install("rpy2")
# Get your python file
source_python("api_clients/aa_summarization.py")
# Get your python file
source_python("api_clients/aa_summarization.py")
py_install("Jinja2")
# Get your python file
source_python("api_clients/aa_summarization.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzExMH0.waXyiiWVTYaiHkpW2fuTcFx9-R67j3_FZBpm0yfEUFg"
# Load PDF file and parse ------------------------------------------------------
pdf_file = "www/Testfile.pdf"
txt = pdf_text(pdf_file)
document = txt[1] # select the page you want to summarize
# Call Aleph Alpha API ---------------------------------------------------------
summary = summary(token, document)
# Put the path to your python environment here
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
# Put the path to your python environment here
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
library(shiny)
library(shinyWidgets)
library(bslib)
library(thematic)
library(reticulate)
library(TheOpenAIR)
library(glue)
library(DT)
library(pdftools)
library(rmarkdown)
# Get your python file
source_python("api_clients/aa_summarization.py")
# Put the path to your python environment here
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
library(pdftools)
library(reticulate)
library(glue)
# Put the path to your python environment here
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
library(pdftools)
library(reticulate)
library(glue)
# Put the path to your python environment here
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("aleph-alpha-client")
py_install("Jinja2")
py_install("numpy")
py_install("rpy2")
# Get your python file
source_python("api_clients/aa_summarization.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzExMH0.waXyiiWVTYaiHkpW2fuTcFx9-R67j3_FZBpm0yfEUFg"
# Load PDF file and parse ------------------------------------------------------
pdf_file = "www/Testfile.pdf"
txt = pdf_text(pdf_file)
document = txt[1] # select the page you want to summarize
document = txt[2] # select the page you want to summarize
document
# Call Aleph Alpha API ---------------------------------------------------------
summary = summary(token, document)
summary
