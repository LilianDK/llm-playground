newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]])
newrows['page'] <- x
colnames(df) = colnames(newrows)
df = rbind(df, newrows)
}
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]])
newrows['page'] <- x
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
View(df)
colnames(df) <- c("Text_chunk","page")
View(df)
#
df = data.frame(matrix(nrow = 0, ncol = 0))
View(df)
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]])
newrows['page'] <- x
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
View(df)
colnames(df) <- c("Text_chunk","page")
View(df)
df = df[!apply(df == "", 1, all), ]
text_chunks = as.list(df)
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks, query, as.integer(3))
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
df = df[!apply(df == "", 1, all), ]
text_chunks = as.list(df)
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
View(df)
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
test = text_chunks[vector]
df <-  as.data.frame(do.call(rbind, test))
View(test)
test = text_chunks[vector]
test = text_chunks[vector][]
df <-  as.data.frame(do.call(rbind, test))
df <-  as.data.frame(do.call(rbind, text_chunks))
View(df)
df <-  as.data.frame(do.call(cbind, text_chunks))
View(df)
df[vector,]
test = df[vector,]
View(test)
library(pdftools)
library(reticulate)
source_python("api_clients/aa_semantic_search_inmemo.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzA4NH0.SyDVglcLc5FLLF86GV9z0D4WfxQF0uvAmeW8YlnG7to"
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]])
newrows['page'] <- x
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
colnames(df) <- c("Text_chunk","page")
df
View(df)
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
# Semantic search
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
results = df[vector,]
View(results)
View(results)
string = ""
for (x in 1:length(df)) {
string = glue("string{df[1,x]}")
}
library(glue)
string = ""
for (x in 1:length(df)) {
string = glue("string{df[1,x]}")
}
string
df[1,1]
df[1,2]
df[2,1]
string = ""
for (x in 1:length(df)) {
string = glue("string{df[x,1]}")
}
string
length(df)
results(df)
length(results)
nrow(results)
string = ""
for (x in 1:nrow(results)) {
string = glue("string{df[x,1]}")
}
string
string = ""
string = glue("string{df[1,1]}")
string
string = ""
for (x in 1:nrow(results)) {
string = glue("{df[1,1]}")
}
string
string = ""
for (x in 1:nrow(results)) {
string = glue("{resutls[2,1]}")
}
string = ""
for (x in 1:nrow(results)) {
string = glue("{results[2,1]}")
}
string
string = ""
for (x in 1:nrow(results)) {
string = glue("{results[x,1]}")
}
string
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
string
summary = summary(token, string)
summary
source_python("api_clients/aa_summarization.py")
summary = summary(token, string)
summary
library(reticulate)
library(pdftools)
library(reticulate)
library(glue)
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
library(pdftools)
library(reticulate)
library(glue)
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzA4NH0.SyDVglcLc5FLLF86GV9z0D4WfxQF0uvAmeW8YlnG7to"
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
colnames(df) <- c("Text_chunk","page")
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
# Semantic search
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
results = df[vector,]
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
clear
clear()
clean()
library(pdftools)
library(reticulate)
library(glue)
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzA4NH0.SyDVglcLc5FLLF86GV9z0D4WfxQF0uvAmeW8YlnG7to"
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
colnames(df) <- c("Text_chunk","page")
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
# Semantic search
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
results = df[vector,]
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
string
qna = qna(token, string, query)
qna = qna(token, string, query)
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
source_python("api_clients/aa_qna.py")
source_python("api_clients/aa_qna.py")
library(pdftools)
library(reticulate)
library(glue)
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzA4NH0.SyDVglcLc5FLLF86GV9z0D4WfxQF0uvAmeW8YlnG7to"
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
colnames(df) <- c("Text_chunk","page")
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
# Semantic search
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
results = df[vector,]
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
string
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
View(tbl)
string
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
gsub(pattern, replacement, string, ignore.case=TRUE/FALSE)
gsub(“[\r\n]”, “”, string)
gsub("[\r\n]", "", string)
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
string = gsub("[\r\n]", "", string)
string
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
reticulate::py_last_error()
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzA4NH0.SyDVglcLc5FLLF86GV9z0D4WfxQF0uvAmeW8YlnG7to"
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
colnames(df) <- c("Text_chunk","page")
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
# Semantic search
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
results = df[vector,]
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
string
string = gsub("[\r\n]", "", string)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
View(qna)
qna[0]
qna$0
View(qna)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
reticulate::py_last_error()
qna[[1]]
qna[1]
View(qna)
qna[[1]]
reticulate::py_last_error()
qna[[0]]
reticulate::py_last_error()
qna[[0][0]]
qna[0][0]
typeof(qna)
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
reticulate::py_last_error()
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
reticulate::py_last_error()
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
source_python("api_clients/aa_qna.py")
qna = qna(token, string, query)
qna
qna[[2]]
localization = qna[[1]]
score = qna[[2]]
nlg = qna[[4]]
localization
score
nlg
nlg = trimws(qna[[4]])
nlg
string = trimws(gsub("[\r\n]", "", string))
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
View(results)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(pdftools)
library(reticulate)
library(glue)
use_python("/Users/lilian.do-khac/.pyenv/versions/3.11.4/bin/python")
py_install("numpy")
py_install("rpy2")
source_python("api_clients/aa_semantic_search_inmemo.py")
source_python("api_clients/aa_qna.py")
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzA4NH0.SyDVglcLc5FLLF86GV9z0D4WfxQF0uvAmeW8YlnG7to"
# Data pre-processing step: Parsing PDF input to input format for semantic search
pdf_file = "www/0.pdf"
txt = pdf_text(pdf_file)
#document = txt[1]
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(as.data.frame(txt))) {
newrows = as.data.frame(strsplit(txt, "\\n\\n")[[x]]) # chunk after each paragraph
newrows['page'] <- x # add page number
colnames(newrows) = colnames(df)
df = rbind(df, newrows)
}
colnames(df) <- c("Text_chunk","page")
df = df[!(is.na(df$Text_chunk) | df$Text_chunk==""), ]
text_chunks = as.list(df)
# Semantic search
query = "Wieviel Euro muss soll die Beklagte zahlen?"
index = semanticsearch(token, text_chunks$Text_chunk, query, as.integer(3))
# Data post-processing step: Transform LLM result to expected tabular output format
tbl = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:length(index)-1) {
newvalue = as.integer(index[x]$item())
tbl = rbind(tbl,newvalue)
}
vector = as.matrix(tbl[1])
results = df[vector,]
string = ""
for (x in 1:nrow(results)) {
string = glue("{string}{results[x,1]}")
}
string = trimws(gsub("[\r\n]", "", string))
qna = qna(token, string, query)
localization = qna[[1]]
score = qna[[2]]
nlg = trimws(qna[[4]])
localization
score
nlg
runApp()
runApp()
